{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWD: /Users/amittaijoel/workspace/transfusion/src/network\n",
      "device = 'cpu'\n"
     ]
    }
   ],
   "source": [
    "#? load scripts from transformer.py\n",
    "import os\n",
    "# os.chdir(\"transfusion\")\n",
    "\n",
    "# print current working directory\n",
    "print(f\"PWD: {os.getcwd()}\")\n",
    "from transformer import device, torch, GPTLanguageModel, TextProcessor\n",
    "\n",
    "print(f\"{device = }\")\n",
    "# os.chdir(\"./\")\n",
    "\n",
    "# import torch\n",
    "# torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.002711 M parameters\n",
      "\n",
      "11.002711 M parameters\n",
      "step     0: train loss 5.9688, val loss 5.9602\n",
      "step   500: train loss 2.4250, val loss 2.3721\n",
      "step  1000: train loss 1.6260, val loss 1.4751\n",
      "step  1500: train loss 1.3847, val loss 1.2461\n",
      "step  2000: train loss 1.2924, val loss 1.1724\n",
      "step  2500: train loss 1.2388, val loss 1.1280\n",
      "step  3000: train loss 1.1839, val loss 1.0821\n",
      "step  3500: train loss 1.1676, val loss 1.0786\n",
      "step  4000: train loss 1.1374, val loss 1.0522\n",
      "step  4500: train loss 1.1100, val loss 1.0254\n",
      "step  5000: train loss 1.0925, val loss 1.0193\n",
      "step  5500: train loss 1.0706, val loss 1.0037\n",
      "step  6000: train loss 1.0722, val loss 1.0156\n",
      "step  6500: train loss 1.0369, val loss 0.9949\n",
      "step  7000: train loss 1.0376, val loss 0.9995\n",
      "step  7500: train loss 1.0180, val loss 0.9810\n",
      "step  8000: train loss 1.0053, val loss 0.9698\n",
      "step  8500: train loss 0.9989, val loss 0.9937\n",
      "step  9000: train loss 0.9894, val loss 0.9643\n",
      "step  9500: train loss 0.9810, val loss 0.9610\n",
      "step  9999: train loss 0.9695, val loss 0.9601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "\n",
    "# load last checkpoint\n",
    "model.load_state_dict(torch.load(\"models/transfusion-9999.pth\", map_location=device))\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "previous_checkpoints = \"\"\"\n",
    "11.002711 M parameters\n",
    "step     0: train loss 5.9688, val loss 5.9602\n",
    "step   500: train loss 2.4250, val loss 2.3721\n",
    "step  1000: train loss 1.6260, val loss 1.4751\n",
    "step  1500: train loss 1.3847, val loss 1.2461\n",
    "step  2000: train loss 1.2924, val loss 1.1724\n",
    "step  2500: train loss 1.2388, val loss 1.1280\n",
    "step  3000: train loss 1.1839, val loss 1.0821\n",
    "step  3500: train loss 1.1676, val loss 1.0786\n",
    "step  4000: train loss 1.1374, val loss 1.0522\n",
    "step  4500: train loss 1.1100, val loss 1.0254\n",
    "step  5000: train loss 1.0925, val loss 1.0193\n",
    "step  5500: train loss 1.0706, val loss 1.0037\n",
    "step  6000: train loss 1.0722, val loss 1.0156\n",
    "step  6500: train loss 1.0369, val loss 0.9949\n",
    "step  7000: train loss 1.0376, val loss 0.9995\n",
    "step  7500: train loss 1.0180, val loss 0.9810\n",
    "step  8000: train loss 1.0053, val loss 0.9698\n",
    "step  8500: train loss 0.9989, val loss 0.9937\n",
    "step  9000: train loss 0.9894, val loss 0.9643\n",
    "step  9500: train loss 0.9810, val loss 0.9610\n",
    "step  9999: train loss 0.9695, val loss 0.9601\n",
    "\"\"\"\n",
    "print(previous_checkpoints)\n",
    "\n",
    "model.to(device)\n",
    "# model.train_model(\"transfusion\")\n",
    "# model.train_model(\"transfusion\", checkpoint=4000)\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is going to be darkned. Byds Girl Bucker to\n",
      "it all making the codes and convinced to today's office?\n",
      "“It was not surprising, you’re doing more who clove and\n",
      "go a\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "from transformer import TextProcessor\n",
    "query = \"OpenAI is going to\"\n",
    "context = torch.tensor( [TextProcessor.encode(query)], dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(model.generate(context, max_new_tokens=150)[0].tolist()))\n",
    "\n",
    "# with open('examples/more.txt', 'w') as f:\n",
    "#   text = decode(model.generate(context, max_new_tokens=10000)[0].tolist())\n",
    "#   f.write(text)\n",
    "\n",
    "generated = model.generate(context, max_new_tokens=150)\n",
    "print(TextProcessor.decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is going to be darkned. Byds Girl Bucker to\n",
      "it all making the codes and convinced to today's office?\n",
      "“It was not surprising, you’re doing more who clove and\n",
      "go all-ong-switchiing was pucked. Obsolutions, the 28-900 years, using its\n",
      "core, money disvertentions are nuts and capital-time rotents to build\n",
      "every dif\n"
     ]
    }
   ],
   "source": [
    "context = generated\n",
    "generated = model.generate(context, max_new_tokens=150)\n",
    "print(TextProcessor.decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is going to be darkned. Byds Girl Bucker to\n",
      "it all making the codes and convinced to today's office?\n",
      "“It was not surprising, you’re doing more who clove and\n",
      "go all-ong-switchiing was pucked. Obsolutions, the 28-900 years, using its\n",
      "core, money disvertentions are nuts and capital-time rotents to build\n",
      "every different thinked faith an entity of the seaf’sort of\n",
      "the world. It was a ununclear politican that information parclia,\n",
      "that deld the others plays employ\n"
     ]
    }
   ],
   "source": [
    "context = generated\n",
    "generated = model.generate(context, max_new_tokens=150)\n",
    "print(TextProcessor.decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is going to be darkned. Byds Girl Bucker to\n",
      "it all making the codes and convinced to today's office?\n",
      "“It was not surprising, you’re doing more who clove and\n",
      "go all-ong-switchiing was pucked. Obsolutions, the 28-900 years, using its\n",
      "core, money disvertentions are nuts and capital-time rotents to build\n",
      "every different thinked faith an entity of the seaf’sort of\n",
      "the world. It was a ununclear politican that information parclia,\n",
      "that deld the others plays employees of the this platform\n",
      "workers in mose-complex significant capabilities for, but it is that\n",
      "what demonted Delair to makes a teaming faction,” Dexrsive said.\n",
      "Culture That Homepage Future Tech Classico Cooperate Guide WIRED is\n",
      "if you ready to bring making more language about Technology\n",
      "Review Insights Contact Advertise with up to 244.49 Atrew Photograph:\n",
      "The valuation Turmoil is the products are plastic by $176\n",
      "AI incalusing for a suprisine for emalication and pieces. The\n",
      "predicts in further of\n"
     ]
    }
   ],
   "source": [
    "context = generated\n",
    "generated = model.generate(context, max_new_tokens=500)\n",
    "print(TextProcessor.decode(generated[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
