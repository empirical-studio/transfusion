{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWD: /Users/amittaijoel/workspace/transfusion/src/network\n",
      "device = 'cpu'\n"
     ]
    }
   ],
   "source": [
    "#? load scripts from transformer.py\n",
    "import os\n",
    "# os.chdir(\"transfusion\")\n",
    "\n",
    "# print current working directory\n",
    "print(f\"PWD: {os.getcwd()}\")\n",
    "from transformer import device, torch, GPTLanguageModel, TextProcessor\n",
    "\n",
    "print(f\"{device = }\")\n",
    "# os.chdir(\"./\")\n",
    "\n",
    "# import torch\n",
    "# torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.002711 M parameters\n",
      "\n",
      "11.002711 M parameters\n",
      "step     0: train loss 5.9688, val loss 5.9602\n",
      "step   500: train loss 2.4250, val loss 2.3721\n",
      "step  1000: train loss 1.6260, val loss 1.4751\n",
      "step  1500: train loss 1.3847, val loss 1.2461\n",
      "step  2000: train loss 1.2924, val loss 1.1724\n",
      "step  2500: train loss 1.2388, val loss 1.1280\n",
      "step  3000: train loss 1.1839, val loss 1.0821\n",
      "step  3500: train loss 1.1676, val loss 1.0786\n",
      "step  4000: train loss 1.1374, val loss 1.0522\n",
      "step  4500: train loss 1.1100, val loss 1.0254\n",
      "step  5000: train loss 1.0925, val loss 1.0193\n",
      "step  5500: train loss 1.0706, val loss 1.0037\n",
      "step  6000: train loss 1.0722, val loss 1.0156\n",
      "step  6500: train loss 1.0369, val loss 0.9949\n",
      "step  7000: train loss 1.0376, val loss 0.9995\n",
      "step  7500: train loss 1.0180, val loss 0.9810\n",
      "step  8000: train loss 1.0053, val loss 0.9698\n",
      "step  8500: train loss 0.9989, val loss 0.9937\n",
      "step  9000: train loss 0.9894, val loss 0.9643\n",
      "step  9500: train loss 0.9810, val loss 0.9610\n",
      "step  9999: train loss 0.9695, val loss 0.9601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "\n",
    "# load last checkpoint\n",
    "model.load_state_dict(torch.load(\"models/transfusion-9999.pth\", map_location=device))\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "previous_checkpoints = \"\"\"\n",
    "11.002711 M parameters\n",
    "step     0: train loss 5.9688, val loss 5.9602\n",
    "step   500: train loss 2.4250, val loss 2.3721\n",
    "step  1000: train loss 1.6260, val loss 1.4751\n",
    "step  1500: train loss 1.3847, val loss 1.2461\n",
    "step  2000: train loss 1.2924, val loss 1.1724\n",
    "step  2500: train loss 1.2388, val loss 1.1280\n",
    "step  3000: train loss 1.1839, val loss 1.0821\n",
    "step  3500: train loss 1.1676, val loss 1.0786\n",
    "step  4000: train loss 1.1374, val loss 1.0522\n",
    "step  4500: train loss 1.1100, val loss 1.0254\n",
    "step  5000: train loss 1.0925, val loss 1.0193\n",
    "step  5500: train loss 1.0706, val loss 1.0037\n",
    "step  6000: train loss 1.0722, val loss 1.0156\n",
    "step  6500: train loss 1.0369, val loss 0.9949\n",
    "step  7000: train loss 1.0376, val loss 0.9995\n",
    "step  7500: train loss 1.0180, val loss 0.9810\n",
    "step  8000: train loss 1.0053, val loss 0.9698\n",
    "step  8500: train loss 0.9989, val loss 0.9937\n",
    "step  9000: train loss 0.9894, val loss 0.9643\n",
    "step  9500: train loss 0.9810, val loss 0.9610\n",
    "step  9999: train loss 0.9695, val loss 0.9601\n",
    "\"\"\"\n",
    "print(previous_checkpoints)\n",
    "\n",
    "model.to(device)\n",
    "# model.train_model(\"transfusion\")\n",
    "# model.train_model(\"transfusion\", checkpoint=4000)\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is going to make I don’t help. So the company thinks a piecture of the first, it should be solided naking waterpless. It looks like to misuge, after the same ste\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "from transformer import TextProcessor\n",
    "query = \"OpenAI is going to\"\n",
    "context = torch.tensor( [TextProcessor.encode(query)], dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(model.generate(context, max_new_tokens=150)[0].tolist()))\n",
    "\n",
    "# with open('examples/more.txt', 'w') as f:\n",
    "#   text = decode(model.generate(context, max_new_tokens=10000)[0].tolist())\n",
    "#   f.write(text)\n",
    "\n",
    "generated = model.generate(context, max_new_tokens=150)\n",
    "print(TextProcessor.decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is going to make I don’t help. So the company thinks a piecture of the first, it should be solided naking waterpless. It looks like to misuge, after the same stem. Although, full, other company hopen and playing with AI and convertes taking in three sizable radonal decades, but they are information and picketu\n"
     ]
    }
   ],
   "source": [
    "context = generated\n",
    "generated = model.generate(context, max_new_tokens=150)\n",
    "print(TextProcessor.decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is going to make I don’t help. So the company thinks a piecture of the first, it should be solided naking waterpless. It looks like to misuge, after the same stem. Although, full, other company hopen and playing with AI and convertes taking in three sizable radonal decades, but they are information and picketup gave to give this to protect platform interet — including lospheral. feet the creenless such for a died by again dust the party of Mira and other pu\n"
     ]
    }
   ],
   "source": [
    "context = generated\n",
    "generated = model.generate(context, max_new_tokens=150)\n",
    "print(TextProcessor.decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI is going to make I don’t help. So the company thinks a piecture of the first, it should be solided naking waterpless. It looks like to misuge, after the same stem. Although, full, other company hopen and playing with AI and convertes taking in three sizable radonal decades, but they are information and picketup gave to give this to protect platform interet — including lospheral. feet the creenless such for a died by again dust the party of Mira and other purch communities, especially powered, farming context wired. We built we this? Most Tianla Venture : The Verge as really daily a digital town through scienci: Discovering the UK Asia, G-legisla croses, implicates acrossembly conspiracy, and growing prevent opposed in an intendet that enhaling finds.\n",
      "In terms on TVW fabr did ago, “We attention for Search Black routers.” As no fills make us as musicenes can learn the top for all time? However, the Chats do is the three-plans have drawing managings \n"
     ]
    }
   ],
   "source": [
    "context = generated\n",
    "generated = model.generate(context, max_new_tokens=500)\n",
    "print(TextProcessor.decode(generated[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
